{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd509fe0-fdfa-480d-98bc-4709bc920beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f6f7464-7dd6-45db-9da7-3e228c19ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 0.8\n",
      "Episode 2: Total Reward = 0.7\n",
      "Episode 3: Total Reward = -5.799999999999992\n",
      "Episode 4: Total Reward = 1\n",
      "Episode 5: Total Reward = -5.8999999999999915\n",
      "Episode 6: Total Reward = 0.6\n",
      "Episode 7: Total Reward = 0.7\n",
      "Episode 8: Total Reward = -0.8000000000000005\n",
      "Episode 9: Total Reward = 1.1102230246251565e-16\n",
      "Episode 10: Total Reward = 0.9\n",
      "Episode 11: Total Reward = -0.9000000000000006\n",
      "Episode 12: Total Reward = -1.4000000000000008\n",
      "Episode 13: Total Reward = -4.599999999999996\n",
      "Episode 14: Total Reward = -1.9000000000000012\n",
      "Episode 15: Total Reward = -18.200000000000003\n",
      "Episode 16: Total Reward = -10.199999999999976\n",
      "Episode 17: Total Reward = 1.1102230246251565e-16\n",
      "Episode 18: Total Reward = 0.4\n",
      "Episode 19: Total Reward = -3.9999999999999982\n",
      "Episode 20: Total Reward = -0.6000000000000003\n",
      "Episode 21: Total Reward = -2.1000000000000014\n",
      "Episode 22: Total Reward = 0.8\n",
      "Episode 23: Total Reward = -3.6999999999999993\n",
      "Episode 24: Total Reward = -0.6000000000000003\n",
      "Episode 25: Total Reward = -2.3000000000000016\n",
      "Episode 26: Total Reward = -2.0000000000000013\n",
      "Episode 27: Total Reward = 0.4\n",
      "Episode 28: Total Reward = 0.10000000000000009\n",
      "Episode 29: Total Reward = -4.099999999999998\n",
      "Episode 30: Total Reward = -2.3000000000000016\n",
      "Episode 31: Total Reward = 0.6\n",
      "Episode 32: Total Reward = -0.7000000000000004\n",
      "Episode 33: Total Reward = -0.30000000000000004\n",
      "Episode 34: Total Reward = -0.7000000000000004\n",
      "Episode 35: Total Reward = -1.600000000000001\n",
      "Episode 36: Total Reward = 0.20000000000000007\n",
      "Episode 37: Total Reward = 0.9\n",
      "Episode 38: Total Reward = -7.699999999999985\n",
      "Episode 39: Total Reward = -6.19999999999999\n",
      "Episode 40: Total Reward = -1.4000000000000008\n",
      "Episode 41: Total Reward = -1.600000000000001\n",
      "Episode 42: Total Reward = 0.30000000000000004\n",
      "Episode 43: Total Reward = -0.40000000000000013\n",
      "Episode 44: Total Reward = 0.8\n",
      "Episode 45: Total Reward = 0.4\n",
      "Episode 46: Total Reward = -3.1000000000000014\n",
      "Episode 47: Total Reward = -2.2000000000000015\n",
      "Episode 48: Total Reward = -2.700000000000002\n",
      "Episode 49: Total Reward = 1.1102230246251565e-16\n",
      "Episode 50: Total Reward = -7.599999999999985\n",
      "Episode 51: Total Reward = -0.19999999999999996\n",
      "Episode 52: Total Reward = -0.19999999999999996\n",
      "Episode 53: Total Reward = 0.7\n",
      "Episode 54: Total Reward = 1\n",
      "Episode 55: Total Reward = -1.5000000000000009\n",
      "Episode 56: Total Reward = 0.5\n",
      "Episode 57: Total Reward = -2.1000000000000014\n",
      "Episode 58: Total Reward = 0.30000000000000004\n",
      "Episode 59: Total Reward = 0.30000000000000004\n",
      "Episode 60: Total Reward = -1.8000000000000012\n",
      "Episode 61: Total Reward = -3.1000000000000014\n",
      "Episode 62: Total Reward = -3.799999999999999\n",
      "Episode 63: Total Reward = 0.30000000000000004\n",
      "Episode 64: Total Reward = -8.499999999999982\n",
      "Episode 65: Total Reward = 1.1102230246251565e-16\n",
      "Episode 66: Total Reward = -2.0000000000000013\n",
      "Episode 67: Total Reward = 0.9\n",
      "Episode 68: Total Reward = -2.2000000000000015\n",
      "Episode 69: Total Reward = 0.30000000000000004\n",
      "Episode 70: Total Reward = 0.7\n",
      "Episode 71: Total Reward = -1.2000000000000006\n",
      "Episode 72: Total Reward = 0.6\n",
      "Episode 73: Total Reward = -0.40000000000000013\n",
      "Episode 74: Total Reward = -0.9000000000000006\n",
      "Episode 75: Total Reward = -5.199999999999994\n",
      "Episode 76: Total Reward = -4.599999999999996\n",
      "Episode 77: Total Reward = -4.799999999999995\n",
      "Episode 78: Total Reward = 1\n",
      "Episode 79: Total Reward = -3.0000000000000018\n",
      "Episode 80: Total Reward = -0.30000000000000004\n",
      "Episode 81: Total Reward = -0.8000000000000005\n",
      "Episode 82: Total Reward = -0.7000000000000004\n",
      "Episode 83: Total Reward = 0.10000000000000009\n",
      "Episode 84: Total Reward = 1.1102230246251565e-16\n",
      "Episode 85: Total Reward = 0.7\n",
      "Episode 86: Total Reward = 0.5\n",
      "Episode 87: Total Reward = -0.5000000000000002\n",
      "Episode 88: Total Reward = -0.8000000000000005\n",
      "Episode 89: Total Reward = -0.40000000000000013\n",
      "Episode 90: Total Reward = 0.20000000000000007\n",
      "Episode 91: Total Reward = 0.6\n",
      "Episode 92: Total Reward = -4.899999999999995\n",
      "Episode 93: Total Reward = 0.8\n",
      "Episode 94: Total Reward = 1\n",
      "Episode 95: Total Reward = -0.5000000000000002\n",
      "Episode 96: Total Reward = -3.799999999999999\n",
      "Episode 97: Total Reward = 1\n",
      "Episode 98: Total Reward = -2.1000000000000014\n",
      "Episode 99: Total Reward = 0.20000000000000007\n",
      "Episode 100: Total Reward = 1\n",
      "Episode 101: Total Reward = -0.09999999999999987\n",
      "Episode 102: Total Reward = -1.700000000000001\n",
      "Episode 103: Total Reward = 1.1102230246251565e-16\n",
      "Episode 104: Total Reward = -0.30000000000000004\n",
      "Episode 105: Total Reward = -0.40000000000000013\n",
      "Episode 106: Total Reward = 1\n",
      "Episode 107: Total Reward = -1.3000000000000007\n",
      "Episode 108: Total Reward = 0.7\n",
      "Episode 109: Total Reward = 0.20000000000000007\n",
      "Episode 110: Total Reward = 0.10000000000000009\n",
      "Episode 111: Total Reward = -0.30000000000000004\n",
      "Episode 112: Total Reward = -0.40000000000000013\n",
      "Episode 113: Total Reward = -2.0000000000000013\n",
      "Episode 114: Total Reward = -2.5000000000000018\n",
      "Episode 115: Total Reward = -2.2000000000000015\n",
      "Episode 116: Total Reward = -10.299999999999976\n",
      "Episode 117: Total Reward = -2.5000000000000018\n",
      "Episode 118: Total Reward = -3.200000000000001\n",
      "Episode 119: Total Reward = 0.20000000000000007\n",
      "Episode 120: Total Reward = -0.30000000000000004\n",
      "Episode 121: Total Reward = -2.600000000000002\n",
      "Episode 122: Total Reward = -5.299999999999994\n",
      "Episode 123: Total Reward = -0.30000000000000004\n",
      "Episode 124: Total Reward = -1.3000000000000007\n",
      "Episode 125: Total Reward = -1.600000000000001\n",
      "Episode 126: Total Reward = -2.600000000000002\n",
      "Episode 127: Total Reward = 0.30000000000000004\n",
      "Episode 128: Total Reward = 1.1102230246251565e-16\n",
      "Episode 129: Total Reward = -0.9000000000000006\n",
      "Episode 130: Total Reward = 0.20000000000000007\n",
      "Episode 131: Total Reward = -1.9000000000000012\n",
      "Episode 132: Total Reward = -2.3000000000000016\n",
      "Episode 133: Total Reward = 0.20000000000000007\n",
      "Episode 134: Total Reward = -2.2000000000000015\n",
      "Episode 135: Total Reward = 0.9\n",
      "Episode 136: Total Reward = -1.1000000000000005\n",
      "Episode 137: Total Reward = -1.600000000000001\n",
      "Episode 138: Total Reward = 0.7\n",
      "Episode 139: Total Reward = 1\n",
      "Episode 140: Total Reward = 1\n",
      "Episode 141: Total Reward = -3.4000000000000004\n",
      "Episode 142: Total Reward = 1.1102230246251565e-16\n",
      "Episode 143: Total Reward = -2.3000000000000016\n",
      "Episode 144: Total Reward = -2.5000000000000018\n",
      "Episode 145: Total Reward = 1\n",
      "Episode 146: Total Reward = 0.7\n",
      "Episode 147: Total Reward = -0.8000000000000005\n",
      "Episode 148: Total Reward = -0.6000000000000003\n",
      "Episode 149: Total Reward = -0.09999999999999987\n",
      "Episode 150: Total Reward = 0.8\n",
      "Episode 151: Total Reward = -1.4000000000000008\n",
      "Episode 152: Total Reward = -1.600000000000001\n",
      "Episode 153: Total Reward = 0.5\n",
      "Episode 154: Total Reward = -1.600000000000001\n",
      "Episode 155: Total Reward = -0.5000000000000002\n",
      "Episode 156: Total Reward = 0.4\n",
      "Episode 157: Total Reward = -0.40000000000000013\n",
      "Episode 158: Total Reward = 0.4\n",
      "Episode 159: Total Reward = -1.700000000000001\n",
      "Episode 160: Total Reward = 1\n",
      "Episode 161: Total Reward = 0.30000000000000004\n",
      "Episode 162: Total Reward = 0.7\n",
      "Episode 163: Total Reward = -0.8000000000000005\n",
      "Episode 164: Total Reward = -0.30000000000000004\n",
      "Episode 165: Total Reward = -0.7000000000000004\n",
      "Episode 166: Total Reward = 0.6\n",
      "Episode 167: Total Reward = 0.5\n",
      "Episode 168: Total Reward = -0.8000000000000005\n",
      "Episode 169: Total Reward = 0.7\n",
      "Episode 170: Total Reward = 1\n",
      "Episode 171: Total Reward = 0.10000000000000009\n",
      "Episode 172: Total Reward = -3.5\n",
      "Episode 173: Total Reward = 0.10000000000000009\n",
      "Episode 174: Total Reward = 0.10000000000000009\n",
      "Episode 175: Total Reward = -0.09999999999999987\n",
      "Episode 176: Total Reward = -2.4000000000000017\n",
      "Episode 177: Total Reward = 1\n",
      "Episode 178: Total Reward = 1\n",
      "Episode 179: Total Reward = 1\n",
      "Episode 180: Total Reward = 0.6\n",
      "Episode 181: Total Reward = 0.9\n",
      "Episode 182: Total Reward = 0.8\n",
      "Episode 183: Total Reward = 0.20000000000000007\n",
      "Episode 184: Total Reward = -3.200000000000001\n",
      "Episode 185: Total Reward = -4.299999999999997\n",
      "Episode 186: Total Reward = -1.2000000000000006\n",
      "Episode 187: Total Reward = -0.09999999999999987\n",
      "Episode 188: Total Reward = 1\n",
      "Episode 189: Total Reward = 0.6\n",
      "Episode 190: Total Reward = -2.1000000000000014\n",
      "Episode 191: Total Reward = 0.6\n",
      "Episode 192: Total Reward = 0.10000000000000009\n",
      "Episode 193: Total Reward = -1.2000000000000006\n",
      "Episode 194: Total Reward = 0.9\n",
      "Episode 195: Total Reward = 1.1102230246251565e-16\n",
      "Episode 196: Total Reward = -1.700000000000001\n",
      "Episode 197: Total Reward = -2.3000000000000016\n",
      "Episode 198: Total Reward = -6.899999999999988\n",
      "Episode 199: Total Reward = 0.9\n",
      "Episode 200: Total Reward = -0.19999999999999996\n",
      "Episode 201: Total Reward = -1.8000000000000012\n",
      "Episode 202: Total Reward = 1\n",
      "Episode 203: Total Reward = -0.19999999999999996\n",
      "Episode 204: Total Reward = -0.6000000000000003\n",
      "Episode 205: Total Reward = 1\n",
      "Episode 206: Total Reward = -0.30000000000000004\n",
      "Episode 207: Total Reward = 0.8\n",
      "Episode 208: Total Reward = 0.4\n",
      "Episode 209: Total Reward = 0.4\n",
      "Episode 210: Total Reward = 0.9\n",
      "Episode 211: Total Reward = 1\n",
      "Episode 212: Total Reward = 0.7\n",
      "Episode 213: Total Reward = 1.1102230246251565e-16\n",
      "Episode 214: Total Reward = 0.7\n",
      "Episode 215: Total Reward = -1.700000000000001\n",
      "Episode 216: Total Reward = 0.7\n",
      "Episode 217: Total Reward = 0.6\n",
      "Episode 218: Total Reward = 0.8\n",
      "Episode 219: Total Reward = 0.4\n",
      "Episode 220: Total Reward = -0.9000000000000006\n",
      "Episode 221: Total Reward = 0.5\n",
      "Episode 222: Total Reward = 0.5\n",
      "Episode 223: Total Reward = -1.2000000000000006\n",
      "Episode 224: Total Reward = 0.8\n",
      "Episode 225: Total Reward = 0.5\n",
      "Episode 226: Total Reward = 0.9\n",
      "Episode 227: Total Reward = -0.7000000000000004\n",
      "Episode 228: Total Reward = 1\n",
      "Episode 229: Total Reward = 0.8\n",
      "Episode 230: Total Reward = -1.5000000000000009\n",
      "Episode 231: Total Reward = 0.10000000000000009\n",
      "Episode 232: Total Reward = -1.1000000000000005\n",
      "Episode 233: Total Reward = 0.6\n",
      "Episode 234: Total Reward = 0.5\n",
      "Episode 235: Total Reward = -0.8000000000000005\n",
      "Episode 236: Total Reward = -0.5000000000000002\n",
      "Episode 237: Total Reward = -0.09999999999999987\n",
      "Episode 238: Total Reward = 0.10000000000000009\n",
      "Episode 239: Total Reward = -0.40000000000000013\n",
      "Episode 240: Total Reward = 0.9\n",
      "Episode 241: Total Reward = -3.0000000000000018\n",
      "Episode 242: Total Reward = -1.4000000000000008\n",
      "Episode 243: Total Reward = 0.6\n",
      "Episode 244: Total Reward = 0.8\n",
      "Episode 245: Total Reward = 0.6\n",
      "Episode 246: Total Reward = -3.1000000000000014\n",
      "Episode 247: Total Reward = -0.09999999999999987\n",
      "Episode 248: Total Reward = -0.7000000000000004\n",
      "Episode 249: Total Reward = 0.7\n",
      "Episode 250: Total Reward = -0.19999999999999996\n",
      "Episode 251: Total Reward = 0.4\n",
      "Episode 252: Total Reward = 0.9\n",
      "Episode 253: Total Reward = -0.9000000000000006\n",
      "Episode 254: Total Reward = 0.9\n",
      "Episode 255: Total Reward = 1\n",
      "Episode 256: Total Reward = 0.6\n",
      "Episode 257: Total Reward = -0.7000000000000004\n",
      "Episode 258: Total Reward = 0.6\n",
      "Episode 259: Total Reward = 0.5\n",
      "Episode 260: Total Reward = 1\n",
      "Episode 261: Total Reward = -0.9000000000000006\n",
      "Episode 262: Total Reward = 1.1102230246251565e-16\n",
      "Episode 263: Total Reward = 0.10000000000000009\n",
      "Episode 264: Total Reward = 0.7\n",
      "Episode 265: Total Reward = 0.7\n",
      "Episode 266: Total Reward = -2.600000000000002\n",
      "Episode 267: Total Reward = 0.10000000000000009\n",
      "Episode 268: Total Reward = 1\n",
      "Episode 269: Total Reward = 0.9\n",
      "Episode 270: Total Reward = -0.9000000000000006\n",
      "Episode 271: Total Reward = 0.10000000000000009\n",
      "Episode 272: Total Reward = -1.4000000000000008\n",
      "Episode 273: Total Reward = 1\n",
      "Episode 274: Total Reward = 0.30000000000000004\n",
      "Episode 275: Total Reward = 0.5\n",
      "Episode 276: Total Reward = 1.1102230246251565e-16\n",
      "Episode 277: Total Reward = -1.2000000000000006\n",
      "Episode 278: Total Reward = 1\n",
      "Episode 279: Total Reward = -1.0000000000000004\n",
      "Episode 280: Total Reward = 0.9\n",
      "Episode 281: Total Reward = 1\n",
      "Episode 282: Total Reward = 0.7\n",
      "Episode 283: Total Reward = -1.2000000000000006\n",
      "Episode 284: Total Reward = -2.2000000000000015\n",
      "Episode 285: Total Reward = 0.30000000000000004\n",
      "Episode 286: Total Reward = -0.5000000000000002\n",
      "Episode 287: Total Reward = 0.30000000000000004\n",
      "Episode 288: Total Reward = -0.19999999999999996\n",
      "Episode 289: Total Reward = 0.9\n",
      "Episode 290: Total Reward = 0.6\n",
      "Episode 291: Total Reward = 0.6\n",
      "Episode 292: Total Reward = -0.8000000000000005\n",
      "Episode 293: Total Reward = -2.2000000000000015\n",
      "Episode 294: Total Reward = 1\n",
      "Episode 295: Total Reward = 0.5\n",
      "Episode 296: Total Reward = 0.20000000000000007\n",
      "Episode 297: Total Reward = -2.600000000000002\n",
      "Episode 298: Total Reward = -2.1000000000000014\n",
      "Episode 299: Total Reward = -0.8000000000000005\n",
      "Episode 300: Total Reward = 0.8\n",
      "Episode 301: Total Reward = 1\n",
      "Episode 302: Total Reward = -2.5000000000000018\n",
      "Episode 303: Total Reward = -3.8999999999999986\n",
      "Episode 304: Total Reward = -1.0000000000000004\n",
      "Episode 305: Total Reward = -0.40000000000000013\n",
      "Episode 306: Total Reward = 0.9\n",
      "Episode 307: Total Reward = -0.19999999999999996\n",
      "Episode 308: Total Reward = 0.9\n",
      "Episode 309: Total Reward = 1.1102230246251565e-16\n",
      "Episode 310: Total Reward = -0.09999999999999987\n",
      "Episode 311: Total Reward = 0.8\n",
      "Episode 312: Total Reward = 0.4\n",
      "Episode 313: Total Reward = 0.5\n",
      "Episode 314: Total Reward = 0.4\n",
      "Episode 315: Total Reward = 0.20000000000000007\n",
      "Episode 316: Total Reward = 0.6\n",
      "Episode 317: Total Reward = -4.899999999999995\n",
      "Episode 318: Total Reward = 0.10000000000000009\n",
      "Episode 319: Total Reward = 0.9\n",
      "Episode 320: Total Reward = 0.6\n",
      "Episode 321: Total Reward = 0.9\n",
      "Episode 322: Total Reward = 0.6\n",
      "Episode 323: Total Reward = -0.09999999999999987\n",
      "Episode 324: Total Reward = -1.2000000000000006\n",
      "Episode 325: Total Reward = -0.8000000000000005\n",
      "Episode 326: Total Reward = 0.9\n",
      "Episode 327: Total Reward = -1.2000000000000006\n",
      "Episode 328: Total Reward = -3.5\n",
      "Episode 329: Total Reward = 0.8\n",
      "Episode 330: Total Reward = -3.4000000000000004\n",
      "Episode 331: Total Reward = 0.30000000000000004\n",
      "Episode 332: Total Reward = 0.9\n",
      "Episode 333: Total Reward = -0.19999999999999996\n",
      "Episode 334: Total Reward = -1.8000000000000012\n",
      "Episode 335: Total Reward = 0.7\n",
      "Episode 336: Total Reward = -1.3000000000000007\n",
      "Episode 337: Total Reward = 0.4\n",
      "Episode 338: Total Reward = -0.9000000000000006\n",
      "Episode 339: Total Reward = 0.9\n",
      "Episode 340: Total Reward = -0.19999999999999996\n",
      "Episode 341: Total Reward = -2.1000000000000014\n",
      "Episode 342: Total Reward = 0.30000000000000004\n",
      "Episode 343: Total Reward = 0.30000000000000004\n",
      "Episode 344: Total Reward = 0.8\n",
      "Episode 345: Total Reward = 1.1102230246251565e-16\n",
      "Episode 346: Total Reward = 0.10000000000000009\n",
      "Episode 347: Total Reward = -0.30000000000000004\n",
      "Episode 348: Total Reward = -0.7000000000000004\n",
      "Episode 349: Total Reward = 0.9\n",
      "Episode 350: Total Reward = 0.4\n",
      "Episode 351: Total Reward = -1.1000000000000005\n",
      "Episode 352: Total Reward = 0.30000000000000004\n",
      "Episode 353: Total Reward = -1.4000000000000008\n",
      "Episode 354: Total Reward = -0.09999999999999987\n",
      "Episode 355: Total Reward = -0.19999999999999996\n",
      "Episode 356: Total Reward = -0.8000000000000005\n",
      "Episode 357: Total Reward = 1.1102230246251565e-16\n",
      "Episode 358: Total Reward = 0.8\n",
      "Episode 359: Total Reward = -1.600000000000001\n",
      "Episode 360: Total Reward = 1.1102230246251565e-16\n",
      "Episode 361: Total Reward = 0.7\n",
      "Episode 362: Total Reward = 0.4\n",
      "Episode 363: Total Reward = -0.09999999999999987\n",
      "Episode 364: Total Reward = 0.7\n",
      "Episode 365: Total Reward = 1\n",
      "Episode 366: Total Reward = -1.0000000000000004\n",
      "Episode 367: Total Reward = 0.7\n",
      "Episode 368: Total Reward = 1\n",
      "Episode 369: Total Reward = 1.1102230246251565e-16\n",
      "Episode 370: Total Reward = -0.6000000000000003\n",
      "Episode 371: Total Reward = -0.9000000000000006\n",
      "Episode 372: Total Reward = 0.20000000000000007\n",
      "Episode 373: Total Reward = 0.9\n",
      "Episode 374: Total Reward = -0.7000000000000004\n",
      "Episode 375: Total Reward = -0.09999999999999987\n",
      "Episode 376: Total Reward = -0.9000000000000006\n",
      "Episode 377: Total Reward = 0.9\n",
      "Episode 378: Total Reward = -0.6000000000000003\n",
      "Episode 379: Total Reward = -0.9000000000000006\n",
      "Episode 380: Total Reward = 1.1102230246251565e-16\n",
      "Episode 381: Total Reward = -0.6000000000000003\n",
      "Episode 382: Total Reward = -0.9000000000000006\n",
      "Episode 383: Total Reward = -0.9000000000000006\n",
      "Episode 384: Total Reward = -1.0000000000000004\n",
      "Episode 385: Total Reward = 1\n",
      "Episode 386: Total Reward = 0.9\n",
      "Episode 387: Total Reward = 0.20000000000000007\n",
      "Episode 388: Total Reward = 0.30000000000000004\n",
      "Episode 389: Total Reward = -0.5000000000000002\n",
      "Episode 390: Total Reward = -0.40000000000000013\n",
      "Episode 391: Total Reward = 0.30000000000000004\n",
      "Episode 392: Total Reward = -0.7000000000000004\n",
      "Episode 393: Total Reward = 0.5\n",
      "Episode 394: Total Reward = -0.19999999999999996\n",
      "Episode 395: Total Reward = -2.5000000000000018\n",
      "Episode 396: Total Reward = 0.30000000000000004\n",
      "Episode 397: Total Reward = 1.1102230246251565e-16\n",
      "Episode 398: Total Reward = 0.30000000000000004\n",
      "Episode 399: Total Reward = -3.0000000000000018\n",
      "Episode 400: Total Reward = 0.20000000000000007\n",
      "Episode 401: Total Reward = 0.10000000000000009\n",
      "Episode 402: Total Reward = -1.600000000000001\n",
      "Episode 403: Total Reward = -2.1000000000000014\n",
      "Episode 404: Total Reward = 0.9\n",
      "Episode 405: Total Reward = -1.3000000000000007\n",
      "Episode 406: Total Reward = 0.7\n",
      "Episode 407: Total Reward = -1.2000000000000006\n",
      "Episode 408: Total Reward = -0.5000000000000002\n",
      "Episode 409: Total Reward = 0.6\n",
      "Episode 410: Total Reward = 0.5\n",
      "Episode 411: Total Reward = 0.6\n",
      "Episode 412: Total Reward = 0.6\n",
      "Episode 413: Total Reward = -0.6000000000000003\n",
      "Episode 414: Total Reward = 0.6\n",
      "Episode 415: Total Reward = -1.4000000000000008\n",
      "Episode 416: Total Reward = 1\n",
      "Episode 417: Total Reward = -0.09999999999999987\n",
      "Episode 418: Total Reward = 0.4\n",
      "Episode 419: Total Reward = 0.9\n",
      "Episode 420: Total Reward = -0.30000000000000004\n",
      "Episode 421: Total Reward = 0.6\n",
      "Episode 422: Total Reward = 0.30000000000000004\n",
      "Episode 423: Total Reward = -0.8000000000000005\n",
      "Episode 424: Total Reward = 0.4\n",
      "Episode 425: Total Reward = -1.5000000000000009\n",
      "Episode 426: Total Reward = -0.30000000000000004\n",
      "Episode 427: Total Reward = 0.10000000000000009\n",
      "Episode 428: Total Reward = 0.9\n",
      "Episode 429: Total Reward = 0.20000000000000007\n",
      "Episode 430: Total Reward = -1.5000000000000009\n",
      "Episode 431: Total Reward = -0.19999999999999996\n",
      "Episode 432: Total Reward = -0.7000000000000004\n",
      "Episode 433: Total Reward = 0.7\n",
      "Episode 434: Total Reward = 0.8\n",
      "Episode 435: Total Reward = 0.8\n",
      "Episode 436: Total Reward = -0.19999999999999996\n",
      "Episode 437: Total Reward = 0.8\n",
      "Episode 438: Total Reward = 0.9\n",
      "Episode 439: Total Reward = 0.30000000000000004\n",
      "Episode 440: Total Reward = 0.20000000000000007\n",
      "Episode 441: Total Reward = -0.7000000000000004\n",
      "Episode 442: Total Reward = -0.09999999999999987\n",
      "Episode 443: Total Reward = -0.30000000000000004\n",
      "Episode 444: Total Reward = 0.20000000000000007\n",
      "Episode 445: Total Reward = 0.10000000000000009\n",
      "Episode 446: Total Reward = -0.40000000000000013\n",
      "Episode 447: Total Reward = 1.1102230246251565e-16\n",
      "Episode 448: Total Reward = -2.700000000000002\n",
      "Episode 449: Total Reward = 0.4\n",
      "Episode 450: Total Reward = 0.9\n",
      "Episode 451: Total Reward = 0.6\n",
      "Episode 452: Total Reward = 0.8\n",
      "Episode 453: Total Reward = -1.2000000000000006\n",
      "Episode 454: Total Reward = -1.1000000000000005\n",
      "Episode 455: Total Reward = -0.30000000000000004\n",
      "Episode 456: Total Reward = -0.40000000000000013\n",
      "Episode 457: Total Reward = 0.30000000000000004\n",
      "Episode 458: Total Reward = -0.19999999999999996\n",
      "Episode 459: Total Reward = -1.0000000000000004\n",
      "Episode 460: Total Reward = 0.5\n",
      "Episode 461: Total Reward = 0.7\n",
      "Episode 462: Total Reward = 0.6\n",
      "Episode 463: Total Reward = -0.5000000000000002\n",
      "Episode 464: Total Reward = 0.5\n",
      "Episode 465: Total Reward = -1.4000000000000008\n",
      "Episode 466: Total Reward = -0.6000000000000003\n",
      "Episode 467: Total Reward = 1.1102230246251565e-16\n",
      "Episode 468: Total Reward = 0.7\n",
      "Episode 469: Total Reward = -0.30000000000000004\n",
      "Episode 470: Total Reward = -1.5000000000000009\n",
      "Episode 471: Total Reward = 1.1102230246251565e-16\n",
      "Episode 472: Total Reward = -0.30000000000000004\n",
      "Episode 473: Total Reward = 0.4\n",
      "Episode 474: Total Reward = -0.5000000000000002\n",
      "Episode 475: Total Reward = 0.5\n",
      "Episode 476: Total Reward = 0.4\n",
      "Episode 477: Total Reward = 1.1102230246251565e-16\n",
      "Episode 478: Total Reward = 1.1102230246251565e-16\n",
      "Episode 479: Total Reward = 0.6\n",
      "Episode 480: Total Reward = -1.0000000000000004\n",
      "Episode 481: Total Reward = 0.5\n",
      "Episode 482: Total Reward = 1.1102230246251565e-16\n",
      "Episode 483: Total Reward = 0.4\n",
      "Episode 484: Total Reward = 0.4\n",
      "Episode 485: Total Reward = 1\n",
      "Episode 486: Total Reward = -0.30000000000000004\n",
      "Episode 487: Total Reward = 0.7\n",
      "Episode 488: Total Reward = 0.6\n",
      "Episode 489: Total Reward = -3.0000000000000018\n",
      "Episode 490: Total Reward = 0.9\n",
      "Episode 491: Total Reward = 0.7\n",
      "Episode 492: Total Reward = -0.30000000000000004\n",
      "Episode 493: Total Reward = 1\n",
      "Episode 494: Total Reward = 0.5\n",
      "Episode 495: Total Reward = 0.10000000000000009\n",
      "Episode 496: Total Reward = -0.19999999999999996\n",
      "Episode 497: Total Reward = 0.30000000000000004\n",
      "Episode 498: Total Reward = 0.20000000000000007\n",
      "Episode 499: Total Reward = -1.3000000000000007\n",
      "Episode 500: Total Reward = 0.7\n",
      ". . R . .\n",
      ". . C . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "\n",
      ". . C R .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "\n",
      ". . R . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "# Custom Environment\n",
    "class TagEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TagEnv, self).__init__()\n",
    "        self.grid_size = 5\n",
    "        self.state = None\n",
    "\n",
    "        # Action space: Up, Down, Left, Right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Observation space: Positions of two blocks\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size - 1, shape=(4,), dtype=np.int32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Player 1 (chaser) and Player 2 (runner) positions\n",
    "        self.state = np.random.randint(0, self.grid_size, size=4)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        chaser_pos = self.state[:2]\n",
    "        runner_pos = self.state[2:]\n",
    "\n",
    "        # Move chaser\n",
    "        chaser_pos = self._move(chaser_pos, action[0])\n",
    "\n",
    "        # Move runner\n",
    "        runner_action = np.random.choice(4)  # Runner moves randomly\n",
    "        runner_pos = self._move(runner_pos, runner_action)\n",
    "\n",
    "        self.state = np.concatenate([chaser_pos, runner_pos])\n",
    "\n",
    "        # Check if chaser caught the runner\n",
    "        done = np.array_equal(chaser_pos, runner_pos)\n",
    "        reward = 1 if done else -0.1\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def _move(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 0:  # Up\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # Down\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "        elif action == 2:  # Left\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # Right\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        return np.array([x, y])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        chaser_pos = tuple(self.state[:2])\n",
    "        runner_pos = tuple(self.state[2:])\n",
    "        grid[chaser_pos] = 'C'\n",
    "        grid[runner_pos] = 'R'\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "# Q-Learning Code\n",
    "# Initialize environment\n",
    "env = TagEnv()\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1    # Learning rate\n",
    "gamma = 0.9    # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "episodes = 500\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((env.grid_size, env.grid_size, env.grid_size, env.grid_size, 4))\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(4)  # Random action\n",
    "    chaser_x, chaser_y, runner_x, runner_y = state\n",
    "    return np.argmax(q_table[chaser_x, chaser_y, runner_x, runner_y])  # Best action\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "\n",
    "        # Update Q-value\n",
    "        chaser_x, chaser_y, runner_x, runner_y = state\n",
    "        next_chaser_x, next_chaser_y, next_runner_x, next_runner_y = next_state\n",
    "        q_table[chaser_x, chaser_y, runner_x, runner_y, action] += alpha * (\n",
    "            reward + gamma * np.max(q_table[next_chaser_x, next_chaser_y, next_runner_x, next_runner_y]) \n",
    "            - q_table[chaser_x, chaser_y, runner_x, runner_y, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Testing\n",
    "state = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "while not done:\n",
    "    action = choose_action(state)\n",
    "    state, _, done, _ = env.step([action])\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0595a412-0930-4089-aa18-9214f22d6a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation Complete!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Custom Tag Environment\n",
    "class TagEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TagEnv, self).__init__()\n",
    "        self.grid_size = 5\n",
    "\n",
    "        # Action space: Up, Down, Left, Right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Observation space: Positions of two blocks\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size - 1, shape=(4,), dtype=np.int32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Player 1 (chaser) and Player 2 (runner) positions\n",
    "        self.state = np.random.randint(0, self.grid_size, size=4)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        chaser_pos = self.state[:2]\n",
    "        runner_pos = self.state[2:]\n",
    "\n",
    "        # Move chaser\n",
    "        chaser_pos = self._move(chaser_pos, action[0])\n",
    "\n",
    "        # Move runner\n",
    "        runner_action = np.random.choice(4)  # Runner moves randomly\n",
    "        runner_pos = self._move(runner_pos, runner_action)\n",
    "\n",
    "        self.state = np.concatenate([chaser_pos, runner_pos])\n",
    "\n",
    "        # Check if chaser caught the runner\n",
    "        done = np.array_equal(chaser_pos, runner_pos)\n",
    "        reward = 1 if done else -0.1\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def _move(self, position, action):\n",
    "        x, y = position\n",
    "        if action == 0:  # Up\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # Down\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "        elif action == 2:  # Left\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # Right\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        return np.array([x, y])\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        chaser_pos = tuple(self.state[:2])\n",
    "        runner_pos = tuple(self.state[2:])\n",
    "        grid[chaser_pos] = 'C'\n",
    "        grid[runner_pos] = 'R'\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(grid == \".\", cmap=\"gray\", origin=\"upper\", alpha=0.7)\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if grid[i, j] == \"C\":\n",
    "                    plt.text(j, i, \"C\", fontsize=20, ha=\"center\", va=\"center\", color=\"blue\")\n",
    "                elif grid[i, j] == \"R\":\n",
    "                    plt.text(j, i, \"R\", fontsize=20, ha=\"center\", va=\"center\", color=\"red\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "# Q-Learning Code\n",
    "# Initialize environment\n",
    "env = TagEnv()\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1    # Learning rate\n",
    "gamma = 0.9    # Discount factor\n",
    "epsilon = 0.3  # Exploration rate\n",
    "episodes = 500\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((env.grid_size, env.grid_size, env.grid_size, env.grid_size, 4))\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(4)  # Random action\n",
    "    chaser_x, chaser_y, runner_x, runner_y = state\n",
    "    return np.argmax(q_table[chaser_x, chaser_y, runner_x, runner_y])  # Best action\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "\n",
    "        # Update Q-value\n",
    "        chaser_x, chaser_y, runner_x, runner_y = state\n",
    "        next_chaser_x, next_chaser_y, next_runner_x, next_runner_y = next_state\n",
    "        q_table[chaser_x, chaser_y, runner_x, runner_y, action] += alpha * (\n",
    "            reward + gamma * np.max(q_table[next_chaser_x, next_chaser_y, next_runner_x, next_runner_y]) \n",
    "            - q_table[chaser_x, chaser_y, runner_x, runner_y, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "# Simulation (Testing)\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    action = choose_action(state)\n",
    "    state, _, done, _ = env.step([action])\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"Simulation Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0c06f-32b0-4ae3-ba65-6320740af596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137222e-cf96-40f5-aafd-27815a72b099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91458ca-77ec-40d3-8c8f-4e316b64fb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
